{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c44ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_agbm(agbm_pred, chipid, config):\n",
    "    im = Image.fromarray(agbm_pred)\n",
    "    save_path = os.path.join(config.DIR_PREDS, f'{chipid}_agbm.tif')\n",
    "    im.save(save_path, format='TIFF', save_all=True)\n",
    "\n",
    "def predict_agbm(inputs, model):\n",
    "    pred = model.predict(inputs.unsqueeze(0))\n",
    "    return pred.detach().squeeze().cpu().numpy()\n",
    "\n",
    "def test(model, dataset_test, config):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for ix, tile in tqdm(enumerate(dataset_test), total=len(dataset_test)):\n",
    "        chipid = dataset_test.df_tile_list.iloc[ix]['chipid']\n",
    "        inputs = tile['image'].to(device)\n",
    "        agbm = predict_agbm(inputs, model)\n",
    "        save_agbm(agbm, chipid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee8e8dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(transforms, hyperparameters):\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"Finland Forests\", tags=['baseline'], config=hyperparameters):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # make the model, data, and optimization problem\n",
    "        model, train_loader, val_loader, loss_module, optimizer, dataset_test = make(transforms, config)\n",
    "\n",
    "        # and use them to train the model\n",
    "        metrics = run_training(model, loss_module, optimizer, train_loader, val_loader, config)\n",
    "        plot_training(metrics)\n",
    "        \n",
    "        # and test its final performance\n",
    "        test(model, dataset_test, config)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cdf2fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = dl.SentinelDataset(\n",
    "                                tile_file=CFG.TILE_FILE_TEST, # specifies best months of test data \n",
    "                                dir_tiles=CFG.DIR_TEST,       # test data dir\n",
    "                                dir_target=None,          # No AGBM targets for test data \n",
    "                                max_chips=CFG.MAX_CHIPS,      \n",
    "                                transform=transforms,     # same transforms as training\n",
    "                                device=loader_device,\n",
    "                                )\n",
    "test(model, dataset_test, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f726c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65eb2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchgeo.transforms import indices\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import dotenv\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning) # biomassters rasters are not georeferenced\n",
    "warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d04527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "os.chdir(os.environ['WORKING_DIR'])\n",
    "import src.utils.transforms as tf\n",
    "import src.utils.data_loader_v3 as dl\n",
    "from config import CFG, CFG2\n",
    "CFG = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efc08f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8241801",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "if torch.backends.mps.is_available(): # Mac M1/M2\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "loader_device = torch.device('cpu')  # found that using cpu for data loading was faster than gpu (for my device)\n",
    "print(f'training device: {device}')\n",
    "print(f'loader_device: {loader_device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "104535a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map of tensor channels *after* transforms, not accounting for bands dropped by the DropBands transform\n",
    "# Useful for choosing which bands to keep \n",
    "band_map = CFG2.BAND_MAP\n",
    "month_map = CFG2.MONTH_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5773eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_to_keep = CFG.BANDS # via offline feature selection  \n",
    "\n",
    "transforms = nn.Sequential(\n",
    "    tf.ClampAGBM(vmin=0., vmax=500.),                # exclude AGBM outliers, 500 is good upper limit per AGBM histograms \n",
    "    indices.AppendNDVI(index_nir=6, index_red=2),    # NDVI, index 15\n",
    "    indices.AppendNormalizedDifferenceIndex(index_a=11, index_b=12), # (VV-VH)/(VV+VH), index 16\n",
    "    indices.AppendNDBI(index_swir=8, index_nir=6),   # Difference Built-up Index for development detection, index 17\n",
    "    indices.AppendNDRE(index_nir=6, index_vre1=3),   # Red Edge Vegetation Index for canopy detection, index 18\n",
    "    indices.AppendNDSI(index_green=1, index_swir=8), # Snow Index, index 19\n",
    "    indices.AppendNDWI(index_green=1, index_nir=6),  # Difference Water Index for water detection, index 20 \n",
    "    indices.AppendSWI(index_vre1=3, index_swir2=8),  # Standardized Water-Level Index for water detection, index 21\n",
    "    tf.AppendRatioAB(index_a=11, index_b=12),        # VV/VH Ascending, index 22\n",
    "    tf.AppendRatioAB(index_a=13, index_b=14),        # VV/VH Descending, index 23\n",
    "    tf.DropBands(loader_device, bands_to_keep)       # DROPS ALL BUT SPECIFIED bands_to_keep\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c936585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    train_metrics = []\n",
    "    \n",
    "    print('Training')\n",
    "    for ix, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        X = batch['image'].to(device)\n",
    "        y = batch['label'].to(device)\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_metrics.append(np.round(np.sqrt(loss.item()), 5))\n",
    "\n",
    "        example_cnt = ix * len(batch)\n",
    "        if ((ix + 1) % 25) == 0:\n",
    "            train_log(train_metrics[-1], example_cnt)\n",
    "            \n",
    "    return train_metrics\n",
    "\n",
    "\n",
    "def valid_loop(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    valid_loss = 0\n",
    "    valid_metrics = {}\n",
    "    \n",
    "    print('Validation')\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, total=num_batches):\n",
    "            X = batch['image'].to(device)\n",
    "            y = batch['label'].to(device)\n",
    "            \n",
    "            pred = model(X)\n",
    "            valid_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "    valid_loss /= num_batches\n",
    "    valid_rmse = np.round(np.sqrt(valid_loss), 5)\n",
    "    print(f\"Validation Error: \\n RMSE: {valid_rmse:>8f} \\n\")\n",
    "    wandb.log({\"test_accuracy\": valid_rmse})\n",
    "    return valid_rmse\n",
    "\n",
    "def train_log(loss, example_ct):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"loss\": loss}, step=example_ct)\n",
    "    print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccd0daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, loss_module, optimizer, train_dataloader, val_dataloader, CFG):\n",
    "    save_file = f'UNET_resnet50_20band_batch{CFG.BATCH_SIZE}_AGBMLinear_AllTrain_{CFG.EPOCHS}epoch_{datetime.now()}.pt'\n",
    "    save_path = os.path.join(CFG.SAVED_MODELS, save_file)\n",
    "    wandb.watch(model, loss_module, log=\"all\", log_freq=10)\n",
    "    min_valid_metric = np.inf\n",
    "    train_metrics = []\n",
    "    valid_metrics = []\n",
    "\n",
    "    for ix in range(CFG.EPOCHS):\n",
    "        print(f\"\\n-------------------------------\\nEpoch {ix+1}\")\n",
    "        train_metrics_epoch = train_loop(train_dataloader, model, loss_module, optimizer)\n",
    "        train_metrics.extend(train_metrics_epoch)\n",
    "        \n",
    "        valid_metrics_epoch = valid_loop(val_dataloader, model, loss_module)\n",
    "        valid_metrics.append((len(train_metrics), valid_metrics_epoch))\n",
    "\n",
    "        # check validation score, if improved then save model\n",
    "        if min_valid_metric > valid_metrics_epoch:\n",
    "            print(f'Validation RMSE Decreased({min_valid_metric:.6f}--->{valid_metrics_epoch:.6f}) \\t Saving The Model')\n",
    "            min_valid_metric = valid_metrics_epoch\n",
    "\n",
    "            # Saving State Dict\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "    print(\"Done!\")\n",
    "    train_metrics_zipped = list(zip(np.arange(0, len(train_metrics)), train_metrics))\n",
    "    \n",
    "    return {'training': train_metrics_zipped, 'validation': valid_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5980c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(transformations, config):\n",
    "\n",
    "    dataset = dl.SentinelDataset(tile_file=config.TILE_FILE, \n",
    "                             dir_tiles=config.DIR_TILES, \n",
    "                             dir_target=config.DIR_TARGET,\n",
    "                             max_chips=config.MAX_CHIPS,\n",
    "                             transform=transforms,\n",
    "                             device=loader_device,\n",
    "                            )\n",
    "    \n",
    "    train_frac = config.TRAIN_FRAC\n",
    "    upper = int(len(dataset)*train_frac)\n",
    "    lower = len(dataset) - upper\n",
    "    train_dataset, val_dataset = random_split(dataset, [upper, lower])\n",
    "    print(f'N training samples: {len(train_dataset)}')\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "                            train_dataset,\n",
    "                            batch_size=config.BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            num_workers=config.WORKERS,\n",
    "                            pin_memory=True\n",
    "                            )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "                            val_dataset,\n",
    "                            batch_size=config.BATCH_SIZE,\n",
    "                            shuffle=False,\n",
    "                            num_workers=config.WORKERS,\n",
    "                            pin_memory=True\n",
    "                            )\n",
    "\n",
    "    in_channels = train_dataset[0]['image'].shape[0]\n",
    "    print(f'# input channels: {in_channels}')\n",
    "\n",
    "    model = smp.Unet(\n",
    "        encoder_name=\"resnet50\",\n",
    "        encoder_weights=None, # 'imagenet' weights don't seem to help so start clean \n",
    "        in_channels=in_channels,                 \n",
    "        classes=1,                     \n",
    "    ).to(device)\n",
    "    #  model.load_state_dict(torch.load(f'trained_models/resnet50-sentinel2.pt'))\n",
    "\n",
    "    loss_module = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.LR)\n",
    "\n",
    "    dataset_test = dl.SentinelDataset(\n",
    "                                    tile_file=config.TILE_FILE_TEST, # specifies best months of test data \n",
    "                                    dir_tiles=config.DIR_TEST,       # test data dir\n",
    "                                    dir_target=None,          # No AGBM targets for test data \n",
    "                                    max_chips=config.MAX_CHIPS,      \n",
    "                                    transform=transforms,     # same transforms as training\n",
    "                                    device=loader_device,\n",
    "                                    )\n",
    "    \n",
    "    return model, train_dataloader, val_dataloader, loss_module, optimizer, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99bb4977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(metrics):\n",
    "    df_train_metrics = pd.DataFrame(metrics['training'], columns=['step', 'score'])\n",
    "    df_valid_metrics = pd.DataFrame(metrics['validation'], columns=['step', 'score'])\n",
    "    plt.plot(df_train_metrics['step'], df_train_metrics['score'], label='Training')\n",
    "    plt.plot(df_valid_metrics['step'], df_valid_metrics['score'], label='Validation')\n",
    "    plt.ylim([0, 100])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f56a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_agbm(agbm_pred, chipid, config):\n",
    "    im = Image.fromarray(agbm_pred)\n",
    "    save_path = os.path.join(config.DIR_PREDS, f'{chipid}_agbm.tif')\n",
    "    im.save(save_path, format='TIFF', save_all=True)\n",
    "\n",
    "def predict_agbm(inputs, model):\n",
    "    pred = model.predict(inputs.unsqueeze(0))\n",
    "    return pred.detach().squeeze().cpu().numpy()\n",
    "\n",
    "def test(model, dataset_test, config):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for ix, tile in tqdm(enumerate(dataset_test), total=len(dataset_test)):\n",
    "        chipid = dataset_test.df_tile_list.iloc[ix]['chipid']\n",
    "        inputs = tile['image'].to(device)\n",
    "        agbm = predict_agbm(inputs, model)\n",
    "        save_agbm(agbm, chipid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b230a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(transforms, hyperparameters):\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"Finland Forests\", tags=['baseline'], config=hyperparameters):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # make the model, data, and optimization problem\n",
    "        model, train_loader, val_loader, loss_module, optimizer, dataset_test = make(transforms, config)\n",
    "\n",
    "        # and use them to train the model\n",
    "        metrics = run_training(model, loss_module, optimizer, train_loader, val_loader, config)\n",
    "        plot_training(metrics)\n",
    "        \n",
    "        # and test its final performance\n",
    "        test(model, dataset_test, config)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74e8836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>"
     ]
    }
   ],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet50\",\n",
    "    encoder_weights=None, # 'imagenet' weights don't seem to help so start clean \n",
    "    in_channels=20,                 \n",
    "    classes=1,                     \n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(f'{CFG.SAVED_MODELS}/UNET_resnet50_20band_batch56_AGBMLinear_AllTrain_5epoch_2023-01-22 15:44:39.035979.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8e0c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = dl.SentinelDataset(\n",
    "                                tile_file=CFG.TILE_FILE_TEST, # specifies best months of test data \n",
    "                                dir_tiles=CFG.DIR_TEST,       # test data dir\n",
    "                                dir_target=None,          # No AGBM targets for test data \n",
    "                                max_chips=CFG.MAX_CHIPS,      \n",
    "                                transform=transforms,     # same transforms as training\n",
    "                                device=loader_device,\n",
    "                                )\n",
    "test(model, dataset_test, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3a73256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_agbm(agbm_pred, chipid, config):\n",
    "    im = Image.fromarray(agbm_pred)\n",
    "    save_path = os.path.join(config.DIR_PREDS, f'{chipid}_agbm.tif')\n",
    "    im.save(save_path, format='TIFF', save_all=True)\n",
    "\n",
    "def predict_agbm(inputs, model):\n",
    "    pred = model.predict(inputs.unsqueeze(0))\n",
    "    return pred.detach().squeeze().cpu().numpy()\n",
    "\n",
    "def test(model, dataset_test, config):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for ix, tile in tqdm(enumerate(dataset_test), total=len(dataset_test)):\n",
    "        chipid = dataset_test.df_tile_list.iloc[ix]['chipid']\n",
    "        inputs = tile['image'].to(device)\n",
    "        agbm = predict_agbm(inputs, model, config)\n",
    "        save_agbm(agbm, chipid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13f0250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = dl.SentinelDataset(\n",
    "                                tile_file=CFG.TILE_FILE_TEST, # specifies best months of test data \n",
    "                                dir_tiles=CFG.DIR_TEST,       # test data dir\n",
    "                                dir_target=None,          # No AGBM targets for test data \n",
    "                                max_chips=CFG.MAX_CHIPS,      \n",
    "                                transform=transforms,     # same transforms as training\n",
    "                                device=loader_device,\n",
    "                                )\n",
    "test(model, dataset_test, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3cd145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_agbm(agbm_pred, chipid, config):\n",
    "    im = Image.fromarray(agbm_pred)\n",
    "    save_path = os.path.join(config.DIR_PREDS, f'{chipid}_agbm.tif')\n",
    "    im.save(save_path, format='TIFF', save_all=True)\n",
    "\n",
    "def predict_agbm(inputs, model):\n",
    "    pred = model.predict(inputs.unsqueeze(0))\n",
    "    return pred.detach().squeeze().cpu().numpy()\n",
    "\n",
    "def test(model, dataset_test, config):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for ix, tile in tqdm(enumerate(dataset_test), total=len(dataset_test)):\n",
    "        chipid = dataset_test.df_tile_list.iloc[ix]['chipid']\n",
    "        inputs = tile['image'].to(device)\n",
    "        agbm = predict_agbm(inputs, model)\n",
    "        save_agbm(agbm, chipid, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59a86044",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = dl.SentinelDataset(\n",
    "                                tile_file=CFG.TILE_FILE_TEST, # specifies best months of test data \n",
    "                                dir_tiles=CFG.DIR_TEST,       # test data dir\n",
    "                                dir_target=None,          # No AGBM targets for test data \n",
    "                                max_chips=CFG.MAX_CHIPS,      \n",
    "                                transform=transforms,     # same transforms as training\n",
    "                                device=loader_device,\n",
    "                                )\n",
    "test(model, dataset_test, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccfff086",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "os.chdir(os.environ['WORKING_DIR'])\n",
    "import src.utils.transforms as tf\n",
    "import src.utils.data_loader_v3 as dl\n",
    "from config import CFG, CFG2\n",
    "CFG = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb505780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6936fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "if torch.backends.mps.is_available(): # Mac M1/M2\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "loader_device = torch.device('cpu')  # found that using cpu for data loading was faster than gpu (for my device)\n",
    "print(f'training device: {device}')\n",
    "print(f'loader_device: {loader_device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbad4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map of tensor channels *after* transforms, not accounting for bands dropped by the DropBands transform\n",
    "# Useful for choosing which bands to keep \n",
    "band_map = CFG2.BAND_MAP\n",
    "month_map = CFG2.MONTH_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c67c1217",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_to_keep = CFG.BANDS # via offline feature selection  \n",
    "\n",
    "transforms = nn.Sequential(\n",
    "    tf.ClampAGBM(vmin=0., vmax=500.),                # exclude AGBM outliers, 500 is good upper limit per AGBM histograms \n",
    "    indices.AppendNDVI(index_nir=6, index_red=2),    # NDVI, index 15\n",
    "    indices.AppendNormalizedDifferenceIndex(index_a=11, index_b=12), # (VV-VH)/(VV+VH), index 16\n",
    "    indices.AppendNDBI(index_swir=8, index_nir=6),   # Difference Built-up Index for development detection, index 17\n",
    "    indices.AppendNDRE(index_nir=6, index_vre1=3),   # Red Edge Vegetation Index for canopy detection, index 18\n",
    "    indices.AppendNDSI(index_green=1, index_swir=8), # Snow Index, index 19\n",
    "    indices.AppendNDWI(index_green=1, index_nir=6),  # Difference Water Index for water detection, index 20 \n",
    "    indices.AppendSWI(index_vre1=3, index_swir2=8),  # Standardized Water-Level Index for water detection, index 21\n",
    "    tf.AppendRatioAB(index_a=11, index_b=12),        # VV/VH Ascending, index 22\n",
    "    tf.AppendRatioAB(index_a=13, index_b=14),        # VV/VH Descending, index 23\n",
    "    tf.DropBands(loader_device, bands_to_keep)       # DROPS ALL BUT SPECIFIED bands_to_keep\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58ed8186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    train_metrics = []\n",
    "    \n",
    "    print('Training')\n",
    "    for ix, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        X = batch['image'].to(device)\n",
    "        y = batch['label'].to(device)\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_metrics.append(np.round(np.sqrt(loss.item()), 5))\n",
    "\n",
    "        example_cnt = ix * len(batch)\n",
    "        if ((ix + 1) % 25) == 0:\n",
    "            train_log(train_metrics[-1], example_cnt)\n",
    "            \n",
    "    return train_metrics\n",
    "\n",
    "\n",
    "def valid_loop(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    valid_loss = 0\n",
    "    valid_metrics = {}\n",
    "    \n",
    "    print('Validation')\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, total=num_batches):\n",
    "            X = batch['image'].to(device)\n",
    "            y = batch['label'].to(device)\n",
    "            \n",
    "            pred = model(X)\n",
    "            valid_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "    valid_loss /= num_batches\n",
    "    valid_rmse = np.round(np.sqrt(valid_loss), 5)\n",
    "    print(f\"Validation Error: \\n RMSE: {valid_rmse:>8f} \\n\")\n",
    "    wandb.log({\"test_accuracy\": valid_rmse})\n",
    "    return valid_rmse\n",
    "\n",
    "def train_log(loss, example_ct):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"loss\": loss}, step=example_ct)\n",
    "    print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "715a025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, loss_module, optimizer, train_dataloader, val_dataloader, CFG):\n",
    "    save_file = f'UNET_resnet50_20band_batch{CFG.BATCH_SIZE}_AGBMLinear_AllTrain_{CFG.EPOCHS}epoch_{datetime.now()}.pt'\n",
    "    save_path = os.path.join(CFG.SAVED_MODELS, save_file)\n",
    "    wandb.watch(model, loss_module, log=\"all\", log_freq=10)\n",
    "    min_valid_metric = np.inf\n",
    "    train_metrics = []\n",
    "    valid_metrics = []\n",
    "\n",
    "    for ix in range(CFG.EPOCHS):\n",
    "        print(f\"\\n-------------------------------\\nEpoch {ix+1}\")\n",
    "        train_metrics_epoch = train_loop(train_dataloader, model, loss_module, optimizer)\n",
    "        train_metrics.extend(train_metrics_epoch)\n",
    "        \n",
    "        valid_metrics_epoch = valid_loop(val_dataloader, model, loss_module)\n",
    "        valid_metrics.append((len(train_metrics), valid_metrics_epoch))\n",
    "\n",
    "        # check validation score, if improved then save model\n",
    "        if min_valid_metric > valid_metrics_epoch:\n",
    "            print(f'Validation RMSE Decreased({min_valid_metric:.6f}--->{valid_metrics_epoch:.6f}) \\t Saving The Model')\n",
    "            min_valid_metric = valid_metrics_epoch\n",
    "\n",
    "            # Saving State Dict\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "    print(\"Done!\")\n",
    "    train_metrics_zipped = list(zip(np.arange(0, len(train_metrics)), train_metrics))\n",
    "    \n",
    "    return {'training': train_metrics_zipped, 'validation': valid_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd4a99cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(transformations, config):\n",
    "\n",
    "    dataset = dl.SentinelDataset(tile_file=config.TILE_FILE, \n",
    "                             dir_tiles=config.DIR_TILES, \n",
    "                             dir_target=config.DIR_TARGET,\n",
    "                             max_chips=config.MAX_CHIPS,\n",
    "                             transform=transforms,\n",
    "                             device=loader_device,\n",
    "                            )\n",
    "    \n",
    "    train_frac = config.TRAIN_FRAC\n",
    "    upper = int(len(dataset)*train_frac)\n",
    "    lower = len(dataset) - upper\n",
    "    train_dataset, val_dataset = random_split(dataset, [upper, lower])\n",
    "    print(f'N training samples: {len(train_dataset)}')\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "                            train_dataset,\n",
    "                            batch_size=config.BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            num_workers=config.WORKERS,\n",
    "                            pin_memory=True\n",
    "                            )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "                            val_dataset,\n",
    "                            batch_size=config.BATCH_SIZE,\n",
    "                            shuffle=False,\n",
    "                            num_workers=config.WORKERS,\n",
    "                            pin_memory=True\n",
    "                            )\n",
    "\n",
    "    in_channels = train_dataset[0]['image'].shape[0]\n",
    "    print(f'# input channels: {in_channels}')\n",
    "\n",
    "    model = smp.Unet(\n",
    "        encoder_name=\"resnet50\",\n",
    "        encoder_weights=None, # 'imagenet' weights don't seem to help so start clean \n",
    "        in_channels=in_channels,                 \n",
    "        classes=1,                     \n",
    "    ).to(device)\n",
    "    #  model.load_state_dict(torch.load(f'trained_models/resnet50-sentinel2.pt'))\n",
    "\n",
    "    loss_module = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.LR)\n",
    "\n",
    "    dataset_test = dl.SentinelDataset(\n",
    "                                    tile_file=config.TILE_FILE_TEST, # specifies best months of test data \n",
    "                                    dir_tiles=config.DIR_TEST,       # test data dir\n",
    "                                    dir_target=None,          # No AGBM targets for test data \n",
    "                                    max_chips=config.MAX_CHIPS,      \n",
    "                                    transform=transforms,     # same transforms as training\n",
    "                                    device=loader_device,\n",
    "                                    )\n",
    "    \n",
    "    return model, train_dataloader, val_dataloader, loss_module, optimizer, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f480a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(metrics):\n",
    "    df_train_metrics = pd.DataFrame(metrics['training'], columns=['step', 'score'])\n",
    "    df_valid_metrics = pd.DataFrame(metrics['validation'], columns=['step', 'score'])\n",
    "    plt.plot(df_train_metrics['step'], df_train_metrics['score'], label='Training')\n",
    "    plt.plot(df_valid_metrics['step'], df_valid_metrics['score'], label='Validation')\n",
    "    plt.ylim([0, 100])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9dcdcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_agbm(agbm_pred, chipid, config):\n",
    "    im = Image.fromarray(agbm_pred)\n",
    "    save_path = os.path.join(config.DIR_PREDS, f'{chipid}_agbm.tif')\n",
    "    im.save(save_path, format='TIFF', save_all=True)\n",
    "\n",
    "def predict_agbm(inputs, model):\n",
    "    pred = model.predict(inputs.unsqueeze(0))\n",
    "    return pred.detach().squeeze().cpu().numpy()\n",
    "\n",
    "def test(model, dataset_test, config):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for ix, tile in tqdm(enumerate(dataset_test), total=len(dataset_test)):\n",
    "        chipid = dataset_test.df_tile_list.iloc[ix]['chipid']\n",
    "        inputs = tile['image'].to(device)\n",
    "        agbm = predict_agbm(inputs, model)\n",
    "        save_agbm(agbm, chipid, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a0a92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = dl.SentinelDataset(\n",
    "                                tile_file=CFG.TILE_FILE_TEST, # specifies best months of test data \n",
    "                                dir_tiles=CFG.DIR_TEST,       # test data dir\n",
    "                                dir_target=None,          # No AGBM targets for test data \n",
    "                                max_chips=CFG.MAX_CHIPS,      \n",
    "                                transform=transforms,     # same transforms as training\n",
    "                                device=loader_device,\n",
    "                                )\n",
    "test(model, dataset_test, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47e30557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2773"
     ]
    }
   ],
   "source": [
    "len(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fce5a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test[2773]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0065c129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[ 0.0777,  0.0793,  0.0843,  ...,  0.0558,  0.0507,  0.0558],\n",
      "          [ 0.0720,  0.0768,  0.0768,  ...,  0.0658,  0.0600,  0.0610],\n",
      "          [ 0.0642,  0.0688,  0.0758,  ...,  0.0608,  0.0567,  0.0523],\n",
      "          ...,\n",
      "          [ 0.0395,  0.0525,  0.0567,  ...,  0.0828,  0.0833,  0.0887],\n",
      "          [ 0.0430,  0.0690,  0.0587,  ...,  0.0950,  0.0875,  0.0828],\n",
      "          [ 0.0653,  0.0765,  0.0632,  ...,  0.0993,  0.0993,  0.0955]],\n",
      " \n",
      "         [[ 0.1205,  0.1262,  0.1180,  ...,  0.0953,  0.0875,  0.0953],\n",
      "          [ 0.1120,  0.1155,  0.1040,  ...,  0.1085,  0.0965,  0.0885],\n",
      "          [ 0.1075,  0.0967,  0.1037,  ...,  0.0920,  0.0908,  0.0890],\n",
      "          ...,\n",
      "          [ 0.0747,  0.0817,  0.0940,  ...,  0.1105,  0.1208,  0.1152],\n",
      "          [ 0.0785,  0.1035,  0.1020,  ...,  0.1287,  0.1213,  0.1190],\n",
      "          [ 0.0965,  0.1217,  0.1010,  ...,  0.1430,  0.1400,  0.1375]],\n",
      " \n",
      "         [[ 0.1117,  0.1217,  0.1377,  ...,  0.0650,  0.0707,  0.0750],\n",
      "          [ 0.1245,  0.1240,  0.1215,  ...,  0.0768,  0.0718,  0.0755],\n",
      "          [ 0.1168,  0.1028,  0.1145,  ...,  0.0785,  0.0758,  0.0712],\n",
      "          ...,\n",
      "          [ 0.0608,  0.0735,  0.0885,  ...,  0.1350,  0.1423,  0.1437],\n",
      "          [ 0.0695,  0.0993,  0.1150,  ...,  0.1612,  0.1472,  0.1443],\n",
      "          [ 0.0925,  0.1357,  0.1343,  ...,  0.1755,  0.1702,  0.1660]],\n",
      " \n",
      "         ...,\n",
      " \n",
      "         [[-0.5936, -0.5783, -0.6294,  ..., -0.5415, -0.5700, -0.5407],\n",
      "          [-0.6168, -0.6071, -0.6660,  ..., -0.4939, -0.5361, -0.5662],\n",
      "          [-0.6409, -0.6709, -0.6608,  ..., -0.5614, -0.5510, -0.5578],\n",
      "          ...,\n",
      "          [-0.6565, -0.6303, -0.6443,  ..., -0.6393, -0.6100, -0.6244],\n",
      "          [-0.6868, -0.6067, -0.6643,  ..., -0.6034, -0.6193, -0.6250],\n",
      "          [-0.6283, -0.5528, -0.6671,  ..., -0.5690, -0.5730, -0.5790]],\n",
      " \n",
      "         [[-0.6315, -0.6126, -0.6096,  ..., -0.6328, -0.6439, -0.6188],\n",
      "          [-0.6361, -0.6286, -0.6454,  ..., -0.6300, -0.6409, -0.6616],\n",
      "          [-0.6309, -0.6518, -0.6358,  ..., -0.6482, -0.6422, -0.6288],\n",
      "          ...,\n",
      "          [-0.6794, -0.6725, -0.6705,  ..., -0.6039, -0.5838, -0.6075],\n",
      "          [-0.6828, -0.6459, -0.6617,  ..., -0.5791, -0.5899, -0.5872],\n",
      "          [-0.6550, -0.6167, -0.6594,  ..., -0.5725, -0.5827, -0.5652]],\n",
      " \n",
      "         [[-0.3414, -0.3414, -0.3838,  ..., -0.3091, -0.3041, -0.3041],\n",
      "          [-0.3414, -0.3414, -0.3838,  ..., -0.3091, -0.3041, -0.3041],\n",
      "          [-0.3547, -0.3547, -0.3790,  ..., -0.3048, -0.2975, -0.2975],\n",
      "          ...,\n",
      "          [-0.3675, -0.3675, -0.3977,  ..., -0.3497, -0.3276, -0.3276],\n",
      "          [-0.3544, -0.3544, -0.4206,  ..., -0.3211, -0.3357, -0.3357],\n",
      "          [-0.3544, -0.3544, -0.4206,  ..., -0.3211, -0.3357, -0.3357]]]),\n",
      " 'label': tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]]])}"
     ]
    }
   ],
   "source": [
    "dataset_test[2772]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3215294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, tile in tqdm(enumerate(dataset_test), total=len(dataset_test)-1):\n",
    "    print(ix)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb8a73c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2773"
     ]
    }
   ],
   "source": [
    "len(os.listdir(CFG.DIR_PREDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6fee424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show predictions\n",
    "model.to(device)\n",
    "\n",
    "tile = sample['image'].to(device)\n",
    "pred = predict_agbm(tile, model)\n",
    "\n",
    "plt.imshow(np.log1p(pred), vmin=vmin, vmax=vmax, interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79d9d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb9e7ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchgeo.transforms import indices\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import dotenv\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning) # biomassters rasters are not georeferenced\n",
    "warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d513fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "os.chdir(os.environ['WORKING_DIR'])\n",
    "import src.utils.transforms as tf\n",
    "import src.utils.data_loader_v3 as dl\n",
    "from config import CFG, CFG2\n",
    "CFG = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92cb493e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "accf13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "if torch.backends.mps.is_available(): # Mac M1/M2\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "loader_device = torch.device('cpu')  # found that using cpu for data loading was faster than gpu (for my device)\n",
    "print(f'training device: {device}')\n",
    "print(f'loader_device: {loader_device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1f3b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map of tensor channels *after* transforms, not accounting for bands dropped by the DropBands transform\n",
    "# Useful for choosing which bands to keep \n",
    "band_map = CFG2.BAND_MAP\n",
    "month_map = CFG2.MONTH_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5aad4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_to_keep = CFG.BANDS # via offline feature selection  \n",
    "\n",
    "transforms = nn.Sequential(\n",
    "    tf.ClampAGBM(vmin=0., vmax=500.),                # exclude AGBM outliers, 500 is good upper limit per AGBM histograms \n",
    "    indices.AppendNDVI(index_nir=6, index_red=2),    # NDVI, index 15\n",
    "    indices.AppendNormalizedDifferenceIndex(index_a=11, index_b=12), # (VV-VH)/(VV+VH), index 16\n",
    "    indices.AppendNDBI(index_swir=8, index_nir=6),   # Difference Built-up Index for development detection, index 17\n",
    "    indices.AppendNDRE(index_nir=6, index_vre1=3),   # Red Edge Vegetation Index for canopy detection, index 18\n",
    "    indices.AppendNDSI(index_green=1, index_swir=8), # Snow Index, index 19\n",
    "    indices.AppendNDWI(index_green=1, index_nir=6),  # Difference Water Index for water detection, index 20 \n",
    "    indices.AppendSWI(index_vre1=3, index_swir2=8),  # Standardized Water-Level Index for water detection, index 21\n",
    "    tf.AppendRatioAB(index_a=11, index_b=12),        # VV/VH Ascending, index 22\n",
    "    tf.AppendRatioAB(index_a=13, index_b=14),        # VV/VH Descending, index 23\n",
    "    tf.DropBands(loader_device, bands_to_keep)       # DROPS ALL BUT SPECIFIED bands_to_keep\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e168477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    train_metrics = []\n",
    "    \n",
    "    print('Training')\n",
    "    for ix, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        X = batch['image'].to(device)\n",
    "        y = batch['label'].to(device)\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_metrics.append(np.round(np.sqrt(loss.item()), 5))\n",
    "\n",
    "        example_cnt = ix * len(batch)\n",
    "        if ((ix + 1) % 25) == 0:\n",
    "            train_log(train_metrics[-1], example_cnt)\n",
    "            \n",
    "    return train_metrics\n",
    "\n",
    "\n",
    "def valid_loop(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    valid_loss = 0\n",
    "    valid_metrics = {}\n",
    "    \n",
    "    print('Validation')\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, total=num_batches):\n",
    "            X = batch['image'].to(device)\n",
    "            y = batch['label'].to(device)\n",
    "            \n",
    "            pred = model(X)\n",
    "            valid_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "    valid_loss /= num_batches\n",
    "    valid_rmse = np.round(np.sqrt(valid_loss), 5)\n",
    "    print(f\"Validation Error: \\n RMSE: {valid_rmse:>8f} \\n\")\n",
    "    wandb.log({\"test_accuracy\": valid_rmse})\n",
    "    return valid_rmse\n",
    "\n",
    "def train_log(loss, example_ct):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"loss\": loss}, step=example_ct)\n",
    "    print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25310269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, loss_module, optimizer, train_dataloader, val_dataloader, CFG):\n",
    "    save_file = f'UNET_resnet50_20band_batch{CFG.BATCH_SIZE}_AGBMLinear_AllTrain_{CFG.EPOCHS}epoch_{datetime.now()}.pt'\n",
    "    save_path = os.path.join(CFG.SAVED_MODELS, save_file)\n",
    "    wandb.watch(model, loss_module, log=\"all\", log_freq=10)\n",
    "    min_valid_metric = np.inf\n",
    "    train_metrics = []\n",
    "    valid_metrics = []\n",
    "\n",
    "    for ix in range(CFG.EPOCHS):\n",
    "        print(f\"\\n-------------------------------\\nEpoch {ix+1}\")\n",
    "        train_metrics_epoch = train_loop(train_dataloader, model, loss_module, optimizer)\n",
    "        train_metrics.extend(train_metrics_epoch)\n",
    "        \n",
    "        valid_metrics_epoch = valid_loop(val_dataloader, model, loss_module)\n",
    "        valid_metrics.append((len(train_metrics), valid_metrics_epoch))\n",
    "\n",
    "        # check validation score, if improved then save model\n",
    "        if min_valid_metric > valid_metrics_epoch:\n",
    "            print(f'Validation RMSE Decreased({min_valid_metric:.6f}--->{valid_metrics_epoch:.6f}) \\t Saving The Model')\n",
    "            min_valid_metric = valid_metrics_epoch\n",
    "\n",
    "            # Saving State Dict\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "    print(\"Done!\")\n",
    "    train_metrics_zipped = list(zip(np.arange(0, len(train_metrics)), train_metrics))\n",
    "    \n",
    "    return {'training': train_metrics_zipped, 'validation': valid_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e239690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(transformations, config):\n",
    "\n",
    "    dataset = dl.SentinelDataset(tile_file=config.TILE_FILE, \n",
    "                             dir_tiles=config.DIR_TILES, \n",
    "                             dir_target=config.DIR_TARGET,\n",
    "                             max_chips=config.MAX_CHIPS,\n",
    "                             transform=transforms,\n",
    "                             device=loader_device,\n",
    "                            )\n",
    "    \n",
    "    train_frac = config.TRAIN_FRAC\n",
    "    upper = int(len(dataset)*train_frac)\n",
    "    lower = len(dataset) - upper\n",
    "    train_dataset, val_dataset = random_split(dataset, [upper, lower])\n",
    "    print(f'N training samples: {len(train_dataset)}')\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "                            train_dataset,\n",
    "                            batch_size=config.BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            num_workers=config.WORKERS,\n",
    "                            prefetch_factor=config.PREFETCH,\n",
    "                            pin_memory=True\n",
    "                            )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "                            val_dataset,\n",
    "                            batch_size=config.BATCH_SIZE,\n",
    "                            shuffle=False,\n",
    "                            num_workers=config.WORKERS,\n",
    "                            prefetch_factor=config.PREFETCH,\n",
    "                            pin_memory=True\n",
    "                            )\n",
    "\n",
    "    in_channels = train_dataset[0]['image'].shape[0]\n",
    "    print(f'# input channels: {in_channels}')\n",
    "\n",
    "    model = smp.Unet(\n",
    "        encoder_name=\"resnet50\",\n",
    "        encoder_weights='imagenet', # 'imagenet' weights don't seem to help so start clean \n",
    "        in_channels=in_channels,                 \n",
    "        classes=1,                     \n",
    "    ).to(device)\n",
    "    #  model.load_state_dict(torch.load(f'trained_models/resnet50-sentinel2.pt'))\n",
    "\n",
    "    loss_module = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.LR)\n",
    "\n",
    "    dataset_test = dl.SentinelDataset(\n",
    "                                    tile_file=config.TILE_FILE_TEST, # specifies best months of test data \n",
    "                                    dir_tiles=config.DIR_TEST,       # test data dir\n",
    "                                    dir_target=None,          # No AGBM targets for test data \n",
    "                                    max_chips=config.MAX_CHIPS,      \n",
    "                                    transform=transforms,     # same transforms as training\n",
    "                                    device=loader_device,\n",
    "                                    )\n",
    "    \n",
    "    return model, train_dataloader, val_dataloader, loss_module, optimizer, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff251e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(metrics):\n",
    "    df_train_metrics = pd.DataFrame(metrics['training'], columns=['step', 'score'])\n",
    "    df_valid_metrics = pd.DataFrame(metrics['validation'], columns=['step', 'score'])\n",
    "    plt.plot(df_train_metrics['step'], df_train_metrics['score'], label='Training')\n",
    "    plt.plot(df_valid_metrics['step'], df_valid_metrics['score'], label='Validation')\n",
    "    plt.ylim([0, 100])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2784386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_agbm(agbm_pred, chipid, config):\n",
    "    im = Image.fromarray(agbm_pred)\n",
    "    save_path = os.path.join(config.DIR_PREDS, f'{chipid}_agbm.tif')\n",
    "    im.save(save_path, format='TIFF', save_all=True)\n",
    "\n",
    "def predict_agbm(inputs, model):\n",
    "    pred = model.predict(inputs.unsqueeze(0))\n",
    "    return pred.detach().squeeze().cpu().numpy()\n",
    "\n",
    "def test(model, dataset_test, config):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for ix, tile in tqdm(enumerate(dataset_test), total=len(dataset_test)-1):\n",
    "        chipid = dataset_test.df_tile_list.iloc[ix]['chipid']\n",
    "        inputs = tile['image'].to(device)\n",
    "        agbm = predict_agbm(inputs, model)\n",
    "        save_agbm(agbm, chipid, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "365f0b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(transforms, hyperparameters):\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"Finland Forests\", tags=['baseline'], config=hyperparameters):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # make the model, data, and optimization problem\n",
    "        model, train_loader, val_loader, loss_module, optimizer, dataset_test = make(transforms, config)\n",
    "\n",
    "        # and use them to train the model\n",
    "        metrics = run_training(model, loss_module, optimizer, train_loader, val_loader, config)\n",
    "        plot_training(metrics)\n",
    "        \n",
    "        # and test its final performance\n",
    "        test(model, dataset_test, config)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3e789d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/broug/Desktop/DD-Finland-Forests/wandb/run-20230122_174235-c8kyps9q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/g-broughton/Finland%20Forests/runs/c8kyps9q\" target=\"_blank\">twilight-pine-6</a></strong> to <a href=\"https://wandb.ai/g-broughton/Finland%20Forests\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/g-broughton/Finland%20Forests\" target=\"_blank\">https://wandb.ai/g-broughton/Finland%20Forests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/g-broughton/Finland%20Forests/runs/c8kyps9q\" target=\"_blank\">https://wandb.ai/g-broughton/Finland%20Forests/runs/c8kyps9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model_pipeline(transforms, CFG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
